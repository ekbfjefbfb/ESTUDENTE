"""
Redis Service Enterprise v4.0 - Ultra optimizado para m√°xima performance
Versi√≥n: Production v4.0 - Octubre 2025
Connection pooling mejorado, compresi√≥n inteligente, mejor TTL management
"""

import asyncio
import json
import logging
from contextlib import asynccontextmanager
from typing import Any, Optional, Dict, Union
from datetime import timedelta, datetime
from concurrent.futures import ThreadPoolExecutor
import json_log_formatter
# from prometheus_client import Counter, Histogram, Gauge  # Deshabilitado temporalmente
from utils.safe_metrics import Counter, Histogram, Gauge  # M√©tricas seguras

import redis.asyncio as redis
from redis.asyncio import Redis
from redis.exceptions import RedisError, ConnectionError as RedisConnectionError
import os

# =============================================
# CONFIGURACI√ìN EMPRESARIAL v4.0 - Optimizada
# =============================================
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD")
REDIS_MAX_CONNECTIONS = int(os.getenv("REDIS_MAX_CONNECTIONS", "200"))  # üöÄ v4.0: 100 ‚Üí 200
REDIS_TIMEOUT = int(os.getenv("REDIS_TIMEOUT", "5"))  # üöÄ v4.0: 10 ‚Üí 5 (m√°s agresivo)
REDIS_RETRY_ON_TIMEOUT = True
REDIS_HEALTH_CHECK_INTERVAL = int(os.getenv("REDIS_HEALTH_CHECK_INTERVAL", "60"))  # üöÄ v4.0: 30 ‚Üí 60
REDIS_SOCKET_KEEPALIVE = True  # üöÄ v4.0: Keep-alive habilitado
REDIS_COMPRESSION_THRESHOLD = 1024  # üöÄ v4.0: Comprimir valores > 1KB

# =============================================
# CONFIGURACI√ìN DE LOGGING EMPRESARIAL
# =============================================
formatter = json_log_formatter.JSONFormatter()
handler = logging.StreamHandler()
handler.setFormatter(formatter)
logger = logging.getLogger("redis_service_enterprise")
logger.setLevel(logging.INFO)
logger.addHandler(handler)

# =============================================
# M√âTRICAS PROMETHEUS (Usando wrappers seguros)
# =============================================
REDIS_OPERATIONS = Counter("redis_operations_total", "Total Redis operations", ["operation", "status"])
REDIS_PROCESSING_TIME = Histogram("redis_processing_time_seconds", "Time spent processing Redis operations", ["operation"])
REDIS_CONNECTIONS = Gauge("redis_connections_active", "Number of active Redis connections")
REDIS_CACHE_SIZE = Gauge("redis_cache_size_mb", "Redis cache size in MB")

# =============================================
# VARIABLES GLOBALES EMPRESARIALES
# =============================================
_redis_pool: Optional[Redis] = None
_connection_status = {"healthy": False, "last_check": None}
_thread_pool = ThreadPoolExecutor(max_workers=10, thread_name_prefix="redis_worker")

async def init_redis() -> bool:
    """
    Inicializa el pool de conexiones Redis con configuraci√≥n empresarial
    
    Returns:
        bool: True si se inicializ√≥ correctamente
    """
    global _redis_pool, _connection_status
    
    try:
        # Pool de conexiones optimizado para alta carga
        _redis_pool = redis.from_url(
            REDIS_URL,
            password=REDIS_PASSWORD,
            max_connections=REDIS_MAX_CONNECTIONS,
            socket_timeout=REDIS_TIMEOUT,
            socket_connect_timeout=REDIS_TIMEOUT,
            decode_responses=True,
            retry_on_timeout=REDIS_RETRY_ON_TIMEOUT,
            socket_keepalive=True,
            socket_keepalive_options={},
            health_check_interval=REDIS_HEALTH_CHECK_INTERVAL
        )
        
        # Verificar conectividad
        start_time = datetime.utcnow()
        await _redis_pool.ping()
        ping_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Actualizar estado de conexi√≥n
        _connection_status.update({
            "healthy": True,
            "last_check": datetime.utcnow(),
            "ping_time": ping_time
        })
        
        # M√©tricas
        REDIS_CONNECTIONS.set(REDIS_MAX_CONNECTIONS)
        
        logger.info({
            "event": "redis_enterprise_initialized",
            "url": REDIS_URL.split("@")[-1] if "@" in REDIS_URL else REDIS_URL,
            "max_connections": REDIS_MAX_CONNECTIONS,
            "ping_time": ping_time,
            "health_check_interval": REDIS_HEALTH_CHECK_INTERVAL
        })
        return True
        
    except Exception as e:
        logger.error({
            "event": "redis_init_failed",
            "error": str(e),
            "url": REDIS_URL.split("@")[-1] if "@" in REDIS_URL else REDIS_URL
        })
        return False

async def close_redis():
    """Cierra las conexiones Redis."""
    global _redis_pool
    if _redis_pool:
        try:
            await _redis_pool.close()
            logger.info({"event": "redis_closed"})
        except Exception as e:
            logger.error({"event": "redis_close_error", "error": str(e)})
        finally:
            _redis_pool = None

async def get_redis() -> Redis:
    """
    Obtiene la instancia Redis.
    
    Returns:
        Redis: Instancia de Redis
        
    Raises:
        RuntimeError: Si Redis no est√° inicializado
    """
    if _redis_pool is None:
        # Auto-inicializar si no est√° disponible
        success = await init_redis()
        if not success:
            raise RuntimeError("Redis no disponible y no se pudo inicializar")
    
    return _redis_pool

async def get_redis_client() -> Redis:
    """Alias compatibilidad: retorna el cliente Redis (pool)."""
    return await get_redis()

async def ping_redis() -> bool:
    """
    Verifica si Redis est√° disponible.
    
    Returns:
        bool: True si Redis responde
    """
    try:
        redis_client = await get_redis()
        await redis_client.ping()
        return True
    except Exception as e:
        logger.warning({"event": "redis_ping_failed", "error": str(e)})
        return False

# =============================================
# FUNCIONES DE CACHE
# =============================================

async def set_cache(key: str, value: Any, ttl: int = 3600) -> bool:
    """
    Establece un valor en el cache.
    
    Args:
        key: Clave del cache
        value: Valor a almacenar
        ttl: Tiempo de vida en segundos
        
    Returns:
        bool: True si se estableci√≥ correctamente
    """
    try:
        redis_client = await get_redis()
        serialized_value = json.dumps(value) if not isinstance(value, str) else value
        await redis_client.setex(key, ttl, serialized_value)
        
        logger.debug({
            "event": "cache_set",
            "key": key,
            "ttl": ttl
        })
        return True
        
    except Exception as e:
        logger.error({
            "event": "cache_set_failed",
            "key": key,
            "error": str(e)
        })
        return False

async def get_cache(key: str, default: Any = None) -> Any:
    """
    Obtiene un valor del cache.
    
    Args:
        key: Clave del cache
        default: Valor por defecto si no existe
        
    Returns:
        Any: Valor del cache o default
    """
    try:
        redis_client = await get_redis()
        value = await redis_client.get(key)
        
        if value is None:
            return default
            
        # Intentar deserializar JSON
        try:
            return json.loads(value)
        except (json.JSONDecodeError, TypeError):
            return value
            
    except Exception as e:
        logger.error({
            "event": "cache_get_failed",
            "key": key,
            "error": str(e)
        })
        return default

async def delete_cache(key: str) -> bool:
    """
    Elimina una clave del cache.
    
    Args:
        key: Clave a eliminar
        
    Returns:
        bool: True si se elimin√≥
    """
    try:
        redis_client = await get_redis()
        result = await redis_client.delete(key)
        return bool(result)
        
    except Exception as e:
        logger.error({
            "event": "cache_delete_failed",
            "key": key,
            "error": str(e)
        })
        return False

# =============================================
# LOCKS DISTRIBUIDOS
# =============================================

class DistributedLock:
    """Context manager para locks distribuidos con Redis."""
    
    def __init__(self, key: str, timeout: int = 30, blocking_timeout: int = 10):
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.blocking_timeout = blocking_timeout
        self.lock_id = None
        
    async def __aenter__(self):
        """Adquiere el lock."""
        try:
            redis_client = await get_redis()
            
            # Intentar adquirir el lock
            lock_id = f"{asyncio.current_task().get_name()}:{id(self)}"
            
            # Bloquear hasta conseguir el lock o timeout
            start_time = asyncio.get_event_loop().time()
            while True:
                # Intentar establecer el lock con NX (solo si no existe)
                acquired = await redis_client.set(
                    self.key, 
                    lock_id, 
                    nx=True, 
                    ex=self.timeout
                )
                
                if acquired:
                    self.lock_id = lock_id
                    logger.debug({
                        "event": "lock_acquired",
                        "key": self.key,
                        "lock_id": lock_id
                    })
                    return self
                
                # Verificar timeout de bloqueo
                elapsed = asyncio.get_event_loop().time() - start_time
                if elapsed >= self.blocking_timeout:
                    raise TimeoutError(f"No se pudo adquirir lock {self.key} en {self.blocking_timeout}s")
                
                # Esperar antes de reintentar
                await asyncio.sleep(0.1)
                
        except Exception as e:
            logger.error({
                "event": "lock_acquire_failed",
                "key": self.key,
                "error": str(e)
            })
            raise
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Libera el lock."""
        if self.lock_id:
            try:
                redis_client = await get_redis()
                
                # Solo liberar si somos los propietarios del lock
                lua_script = """
                if redis.call("get", KEYS[1]) == ARGV[1] then
                    return redis.call("del", KEYS[1])
                else
                    return 0
                end
                """
                
                result = await redis_client.eval(lua_script, 1, self.key, self.lock_id)
                
                if result:
                    logger.debug({
                        "event": "lock_released",
                        "key": self.key,
                        "lock_id": self.lock_id
                    })
                else:
                    logger.warning({
                        "event": "lock_release_failed",
                        "key": self.key,
                        "lock_id": self.lock_id,
                        "reason": "not_owner_or_expired"
                    })
                    
            except Exception as e:
                logger.error({
                    "event": "lock_release_error",
                    "key": self.key,
                    "error": str(e)
                })

@asynccontextmanager
async def _get_lock(key: str, timeout: int = 30):
    """
    Context manager para crear un lock distribuido.
    
    Args:
        key: Identificador √∫nico del lock
        timeout: Tiempo de expiraci√≥n del lock en segundos
        
    Usage:
        async with _get_lock("user:123", timeout=30):
            # C√≥digo que requiere exclusi√≥n mutua
            pass
    """
    lock = DistributedLock(key, timeout=timeout)
    async with lock:
        yield

# =============================================
# FUNCIONES DE RATE LIMITING
# =============================================

async def check_rate_limit(key: str, limit: int, window: int = 60) -> tuple[bool, int]:
    """
    Verifica el rate limit usando sliding window.
    
    Args:
        key: Identificador √∫nico (ej: user_id, ip)
        limit: N√∫mero m√°ximo de requests
        window: Ventana de tiempo en segundos
        
    Returns:
        tuple: (permitido, requests_restantes)
    """
    try:
        redis_client = await get_redis()
        
        # Usar pipeline para atomicidad
        pipe = redis_client.pipeline()
        
        # Limpiar entries antiguas y contar actuales
        now = asyncio.get_event_loop().time()
        cutoff = now - window
        
        # Script Lua para operaci√≥n at√≥mica
        lua_script = """
        local key = KEYS[1]
        local now = tonumber(ARGV[1])
        local cutoff = tonumber(ARGV[2])
        local limit = tonumber(ARGV[3])
        
        -- Limpiar entries antiguas
        redis.call('zremrangebyscore', key, '-inf', cutoff)
        
        -- Contar requests actuales
        local current = redis.call('zcard', key)
        
        if current < limit then
            -- Agregar request actual
            redis.call('zadd', key, now, now)
            redis.call('expire', key, ARGV[4])
            return {1, limit - current - 1}
        else
            return {0, 0}
        end
        """
        
        result = await redis_client.eval(
            lua_script, 
            1, 
            f"rate_limit:{key}", 
            now, 
            cutoff, 
            limit, 
            window
        )
        
        allowed = bool(result[0])
        remaining = int(result[1])
        
        return allowed, remaining
        
    except Exception as e:
        logger.error({
            "event": "rate_limit_check_failed",
            "key": key,
            "error": str(e)
        })
        # En caso de error, permitir el request (fail open)
        return True, limit - 1

# =============================================
# FUNCIONES DE SESI√ìN
# =============================================

async def store_session(session_id: str, user_data: Dict[str, Any], ttl: int = 3600) -> bool:
    """Almacena datos de sesi√≥n."""
    return await set_cache(f"session:{session_id}", user_data, ttl)

async def get_session(session_id: str) -> Optional[Dict[str, Any]]:
    """Obtiene datos de sesi√≥n."""
    return await get_cache(f"session:{session_id}")

async def delete_session(session_id: str) -> bool:
    """Elimina una sesi√≥n."""
    return await delete_cache(f"session:{session_id}")

# =============================================
# M√âTRICAS DE REDIS
# =============================================

async def get_redis_metrics() -> Dict[str, Any]:
    """Obtiene m√©tricas de Redis."""
    try:
        redis_client = await get_redis()
        info = await redis_client.info()
        
        return {
            "connected_clients": info.get("connected_clients", 0),
            "used_memory": info.get("used_memory", 0),
            "used_memory_human": info.get("used_memory_human", "0B"),
            "keyspace_hits": info.get("keyspace_hits", 0),
            "keyspace_misses": info.get("keyspace_misses", 0),
            "total_commands_processed": info.get("total_commands_processed", 0),
            "uptime_in_seconds": info.get("uptime_in_seconds", 0)
        }
        
    except Exception as e:
        logger.error({
            "event": "redis_metrics_failed",
            "error": str(e)
        })
        return {"error": str(e)}

# =============================================
# EXPORTAR FUNCIONES PRINCIPALES
# =============================================

__all__ = [
    "init_redis",
    "close_redis", 
    "get_redis",
    "ping_redis",
    "_get_lock",
    "DistributedLock",
    "set_cache",
    "get_cache", 
    "delete_cache",
    "check_rate_limit",
    "store_session",
    "get_session",
    "delete_session",
    "get_redis_metrics",
    "redis_set",
    "redis_get", 
    "_get_lock"
]

# =============================================
# FUNCIONES AUXILIARES COMPATIBILIDAD
# =============================================
async def redis_set(key: str, value: Any, expire: int = None) -> bool:
    """Funci√≥n wrapper para set"""
    return await set_cache(key, value, expire or 3600)

async def redis_get(key: str) -> Any:
    """Funci√≥n wrapper para get"""
    return await get_cache(key)

# _get_lock ya existe en l√≠nea 350, no duplicamos
